# -*- coding: utf-8 -*-
"""Keras_Inception_V3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EJOx3iVKnFd0zFRhvYxh2O6Zpdpub5Ah

# Connect with Google Drive for Google Colab
"""

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

import os 

print(os.getcwd())
print(os.listdir())

os.chdir('drive/My Drive/Projects/Dog Breed 2.0/')

"""# Import required libraries"""

import numpy as np
from keras import regularizers
import tensorflow.keras as keras
from keras import backend as K
from keras.models import Model, Sequential
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, Dropout
from keras.models import Sequential
from keras.layers import Activation, Lambda, GlobalAveragePooling2D
from keras.layers.core import Dense, Flatten, Dropout
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
from keras.applications.inception_v3 import InceptionV3
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from collections import Counter

"""# Preprocessing"""

train_path = 'dataset/without_downsampled_data/train/'
cls_ = os.listdir(train_path)
no_cls = len(os.listdir(train_path))

img_height = 229
img_width = 229
batch_size = 128
epoch = 100
model_name = "inceptionV3_KaggelDataset_WithoutDownsampled_Droupout0_LrNONE_.h5"


train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   validation_split=0.1)


train_batches = train_datagen.flow_from_directory(
                                                train_path, 
                                                target_size=(img_height, img_width),
                                                classes=cls_, 
                                                batch_size=batch_size,
                                                subset='training')


validation_batches = train_datagen.flow_from_directory(
                                                    train_path,
                                                    target_size=(img_height, img_width),
                                                    classes=cls_,
                                                    batch_size=batch_size,
                                                    subset='validation')


counter = Counter(train_batches.classes)
max_value = float(max(counter.values()))


CLASS_WEIGHTS = {classid: max_value / num_occurences
                 for classid, num_occurences in counter.items()}

"""# Transfer Learning"""

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))

base_model.trainable = False

model = Sequential()

model.add(base_model)
model.add(GlobalAveragePooling2D())
model.add(Dense(1024, activation='relu'))


# enable if required
# model.add(Dense(1024, activation='relu', kernel_regularizer = regularizers.l2(0.0001), 
#                                     activity_regularizer = regularizers.l1(0.0001)))
#model.add(Dropout(0.20))
model.add(Dense(no_cls, activation='softmax'))

print(model.summary())

model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])

print("Model Compiled Successfully")

modelcheckpoint = ModelCheckpoint('Outputs/keras/'+ model_name,
                                  monitor='val_acc',
                                  mode='max',
                                  verbose=1,
                                  save_best_only=True)


lr_callback = ReduceLROnPlateau(min_lr=0.000001)


callback_list = [modelcheckpoint, lr_callback]


history = model.fit_generator(
    train_batches, steps_per_epoch = train_batches.samples // batch_size,
    validation_data=validation_batches, validation_steps = validation_batches.samples // batch_size,
    epochs=epoch, verbose=1, 
    callbacks = callback_list,
    class_weight = CLASS_WEIGHTS,
)

"""# Graphs"""

from keras.callbacks import History

print(history.history.keys())

# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
